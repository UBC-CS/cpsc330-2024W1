{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 9: Class demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), (\"..\"), \"code\"))\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Image\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"..\"), (\"..\"), \"data/\")\n",
    "\n",
    "# Ignore future deprecation warnings from sklearn (using `os` instead of `warnings` also works in subprocesses)\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS']='ignore::FutureWarning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Changing global matplotlib settings for confusion matrix.\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning workflow \n",
    "\n",
    "- Here is a typical workflow of a supervised machine learning systems. \n",
    "- So far, we have talked about data splitting, preprocessing, some EDA, model selection with hyperparameter optimization, and interpretation in the context of linear models.\n",
    "- In the next few lectures, we will talk about evaluation metrics and model selection in terms of evaluation metrics, feature engineering, feature selection, and model transparency and interpretation.  \n",
    "\n",
    "![](../../img/ml-workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics for binary classification: Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset for demonstration \n",
    "\n",
    "- Let's classify fraudulent and non-fraudulent transactions using Kaggle's [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) data set.\n",
    "    - Note, that the credit card fraud detection data set is very large (150mb!) so we've stored it using [GitLFS](https://josh-ops.com/posts/add-files-to-git-lfs/) in a personal repo. You should download the data locally to follow-along, and be sure not to commit large data files to your repository!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# This dataset will be loaded using a URL instead of a CSV file\n",
    "DATA_URL = \"https://github.com/firasm/bits/raw/refs/heads/master/creditcard.csv\"\n",
    "\n",
    "cc_df = pd.read_csv(DATA_URL, encoding=\"latin-1\")\n",
    "# Sorting columns so it is easier to read\n",
    "cc_df = cc_df[['Class', 'Time', 'Amount'] + cc_df.columns[cc_df.columns.str.startswith('V')].to_list()]\n",
    "\n",
    "train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)\n",
    "\n",
    "train_df\n",
    "\n",
    "train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good size dataset \n",
    "- For confidentially reasons, it only provides transformed features with PCA, which is a popular dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We do not have categorical features. All features are numeric. \n",
    "- We have to be careful about the `Time` and `Amount` features. \n",
    "- We could scale `Amount`. \n",
    "- Do we want to scale time?\n",
    "    - In this lecture, we'll just drop the Time feature. \n",
    "    - We'll learn about time series briefly later in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's separate `X` and `y` for train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_big, y_train_big = train_df.drop(columns=[\"Class\", \"Time\"]), train_df[\"Class\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"Class\", \"Time\"]), test_df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- It's easier to demonstrate evaluation metrics using an explicit validation set instead of using cross-validation. \n",
    "- So let's create a validation set. \n",
    "- Our data is large enough so it shouldn't be a problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_big, y_train_big, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "pd.DataFrame(cross_validate(dummy, X_train, y_train, return_train_score=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations \n",
    "\n",
    "- `DummyClassifier` is getting 0.998 cross-validation accuracy!! \n",
    "- Should we be happy with this accuracy and deploy this `DummyClassifier` model for fraud detection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What's the class distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We have class imbalance. \n",
    "- We have MANY non-fraud transactions and only a handful of fraud transactions. \n",
    "- So in the training set, `most_frequent` strategy is labeling 199,025 (99.83%) instances correctly and only 339 (0.17%) instances incorrectly. \n",
    "- Is this what we want? \n",
    "- The \"fraud\" class is the important class that we want to spot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's scale the features and try `LogisticRegression`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pd.DataFrame(cross_validate(pipe, X_train, y_train, return_train_score=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We are getting a slightly better score with logistic regression.  \n",
    "- What score should be considered an acceptable score here? \n",
    "- Are we actually spotting any \"fraud\" transactions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `.score` by default returns accuracy which is \n",
    "$$\\frac{\\text{correct predictions}}{\\text{total examples}}$$\n",
    "- Is accuracy a good metric here? \n",
    "- Is there anything more informative than accuracy that we can use here? \n",
    "\n",
    "Let's dig a little deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding \n",
    "\n",
    "- We have a logistic regression model for fraud detection that predicts a value between 0 and 1, representing the probability that a given email is spam.\n",
    "- We use thresholding to get the binary prediction. \n",
    "- A typical threshold is 0.5.\n",
    "    - A prediction of 0.90 $\\rightarrow$ a high likelihood that the transaction is fraudulent and we predict **fraud**\n",
    "    - A prediction of 0.20 $\\rightarrow$ a low likelihood that the transaction is non-fraudulent and we predict **Non fraud**\n",
    "- **What happens if the predicted score is equal to the chosen threshold?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Create a link to the page\n",
    "link = HTML(\"<a href='https://developers.google.com/machine-learning/crash-course/classification/thresholding' target='_blank'>Visit the Classification Thresholding Interactive Page</a>\")\n",
    "display(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way to get a better understanding of the errors is by looking at \n",
    "- false positives (type I errors), where the model incorrectly spots examples as fraud\n",
    "- false negatives (type II errors), where it's missing to spot fraud examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay  \n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "cm = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"Fraud\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which types of errors would be most critical for the bank to address?\n",
    "\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "plot_confusion_matrix_example(TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Perfect prediction has all values down the diagonal\n",
    "- Off diagonal entries can often tell us about what is being mis-predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is \"positive\" and \"negative\"?\n",
    "\n",
    "- Two kinds of binary classification problems \n",
    "    - Distinguishing between two classes\n",
    "    - Spotting a class (spot fraud transaction, spot spam, spot disease)\n",
    "- In case of spotting problems, the thing that we are interested in spotting is considered \"positive\". \n",
    "- Above we wanted to spot fraudulent transactions and so they are \"positive\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can get a numpy array of confusion matrix as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "print(\"Confusion matrix for fraud data set\")\n",
    "print(cm.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion matrix with cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- You can also calculate confusion matrix with cross-validation using the `cross_val_predict` method.  \n",
    "- But then you cannot plot it in a nice format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "confusion_matrix(y_train, cross_val_predict(pipe, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision, recall, f1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We have been using `.score` to assess our models, which returns accuracy by default. \n",
    "- Accuracy is misleading when we have class imbalance.\n",
    "- We need other metrics to assess our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll discuss three commonly used metrics which are based on confusion matrix: \n",
    "    - recall\n",
    "    - precision\n",
    "    - f1 score \n",
    "- Note that these metrics will only help us assess our model.  \n",
    "- Later we'll talk about a few ways to address class imbalance problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall: toy example\n",
    "- Imagine that your model has identified everything outside the circle as non-fraud and everything inside the circle as fraud. \n",
    "\n",
    "![](../../img/precision-recall.png)\n",
    "\n",
    "![](../../img/fraud-precision-recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "predictions = pipe_lr.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "print(cm.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision \n",
    "\n",
    "Among the positive examples you identified, how many were actually positive?\n",
    "\n",
    "$$ precision = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cm = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"fraud\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FP = %0.4f\" % (TP, FP))\n",
    "precision = TP / (TP + FP)\n",
    "print(\"Precision: %0.4f\" % (precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall \n",
    "\n",
    "Among all positive examples, how many did you identify correctly?\n",
    "$$ recall = \\frac{TP}{TP+FN} = \\frac{TP}{\\#positives} $$\n",
    "\n",
    "- Also called as sensitivity, coverage, true positive rate (TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"fraud\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FN = %0.4f\" % (TP, FN))\n",
    "recall = TP / (TP + FN)\n",
    "print(\"Recall: %0.4f\" % (recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F1-score\n",
    "\n",
    "- F1-score combines precision and recall to give one score, which could be used in hyperparameter optimization, for instance. \n",
    "- F1-score is a harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$ f1 = 2 \\times \\frac{ precision \\times recall}{precision + recall}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"precision: %0.4f\" % (precision))\n",
    "print(\"recall: %0.4f\" % (recall))\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"f1: %0.4f\" % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at all metrics at once on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate evaluation metrics by ourselves\n",
    "data = {\n",
    "    \"calculation\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"error\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1 score\": [],\n",
    "}\n",
    "data[\"calculation\"].append(\"manual\")\n",
    "data[\"accuracy\"].append((TP + TN) / (TN + FP + FN + TP))\n",
    "data[\"error\"].append((FP + FN) / (TN + FP + FN + TP))\n",
    "data[\"precision\"].append(precision)  # TP / (TP + FP)\n",
    "data[\"recall\"].append(recall)  # TP / (TP + FN)\n",
    "data[\"f1 score\"].append(f1_score)  # (2 * precision * recall) / (precision + recall)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `scikit-learn` has functions for [these metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "data[\"accuracy\"].append(accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"error\"].append(1 - accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"precision\"].append(\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid), zero_division=1)\n",
    ")\n",
    "data[\"recall\"].append(recall_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"f1 score\"].append(f1_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"calculation\"].append(\"sklearn\")\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index([\"calculation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a convenient function called `classification_report` in `sklearn` which gives this info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_valid, pipe_lr.predict(X_valid), target_names=[\"non-fraud\", \"fraud\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary \n",
    "\n",
    "- Accuracy is misleading when you have class imbalance. \n",
    "- A confusion matrix provides a way to break down errors made by our model. \n",
    "- We looked at three metrics based on confusion matrix: \n",
    "    - precision, recall, f1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that what you consider \"positive\" (fraud in our case) is important when calculating precision, recall, and f1-score. \n",
    "- If you flip what is considered positive or negative, we'll end up with different TP, FP, TN, FN, and hence different precision, recall, and f1-scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evalution metrics overview  \n",
    "There is a lot of terminology here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../../img/evaluation-metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation with different metrics\n",
    "\n",
    "- We can pass different evaluation metrics with `scoring` argument of `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scoring = [\n",
    "    \"accuracy\",\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "]  # scoring can be a string, a list, or a dictionary\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scores = cross_validate(\n",
    "    pipe, X_train_big, y_train_big, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can also create [your own scoring function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and pass it to `cross_validate`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Confusion matrix provides a detailed break down of the errors made by the model. \n",
    "- But when creating a confusion matrix, we are using \"hard\" predictions. \n",
    "- Most classifiers in `scikit-learn` provide `predict_proba` method (or `decision_function`) which provides degree of certainty about predictions by the classifier. \n",
    "- Can we explore the degree of uncertainty to understand and improve the model performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's revisit the classification report on our fraud detection example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By default, predictions use the threshold of 0.5. If `predict_proba` > 0.5, predict \"fraud\" else predict \"non-fraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] > 0.50\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose for your business it is more costly to miss fraudulent transactions and suppose you want to achieve a recall of at least 75% for the \"fraud\" class. \n",
    "- One way to do this is by changing the threshold of `predict_proba`.\n",
    "    - `predict` returns 1 when `predict_proba`'s probabilities are above 0.5 for the \"fraud\" class.\n",
    "\n",
    "**Key idea: what if we threshold the probability at a smaller value so that we identify more examples as \"fraud\" examples?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's lower the threshold to 0.1. In other words, predict the examples as \"fraud\" if `predict_proba` > 0.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = pipe_lr.predict_proba(X_valid)[:, 1] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Operating point \n",
    "\n",
    "- Now our recall for \"fraud\" class is >= 0.75. \n",
    "- Setting a requirement on a classifier (e.g., recall of >= 0.75) is called setting the **operating point**. \n",
    "- It's usually driven by business goals and is useful to make performance guarantees to customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision/Recall tradeoff \n",
    "\n",
    "- But there is a trade-off between precision and recall. \n",
    "- If you identify more things as \"fraud\", recall is going to increase but there are likely to be more false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's sweep through different thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install `panel` package in order to run the code below locally. See the documentation [here](https://pyviz-dev.github.io/panel/getting_started/installation.html#jupyterlab-and-classic-notebook). \n",
    "\n",
    "```conda install -c pyviz panel```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "from panel import widgets\n",
    "from panel.interact import interact\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "def f(threshold):\n",
    "    preds = pipe_lr.predict_proba(X_valid)[:, 1] > threshold\n",
    "    precision = np.round(precision_score(y_valid, preds), 4)\n",
    "    recall = np.round(recall_score(y_valid, preds), 4)\n",
    "    d = {'threshold':np.round(threshold, 4), 'Precision': precision, 'recall': recall}\n",
    "    return (d)\n",
    "\n",
    "interact(f, threshold=widgets.FloatSlider(start=0.0, end=0.99, step=0.05, value=0.5)).embed(max_opts=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decreasing the threshold\n",
    "\n",
    "- Decreasing the threshold means a lower bar for predicting fraud. \n",
    "    - You are willing to risk more false positives in exchange of more true positives. \n",
    "    - Recall would either stay the same or go up and precision is likely to go down\n",
    "    - Occasionally, precision may increase if all the new examples after decreasing the threshold are TPs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Increasing the threshold\n",
    "\n",
    "- Increasing the threshold means a higher bar for predicting fraud. \n",
    "    - Recall would go down or stay the same but precision is likely to go up \n",
    "    - Occasionally, precision may go down if TP decrease but FP do not decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision-recall curve\n",
    "\n",
    "Often, when developing a model, it's not always clear what the operating point will be and to understand the model better, it's informative to look at all possible thresholds and corresponding trade-offs of precision and recall in a plot.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")\n",
    "plt.plot(precision, recall, label=\"logistic regression: PR curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "    recall_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each point in the curve corresponds to a possible threshold of the `predict_proba` output. \n",
    "- We can achieve a recall of 0.8 at a precision of 0.4. \n",
    "- The red dot marks the point corresponding to the threshold 0.5.\n",
    "- The top-right would be a perfect classifier (precision = recall = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The threshold is not shown here, but it's going from 0 (upper-left) to 1 (lower right).\n",
    "- At a threshold of 0 (upper left), we are classifying everything  as \"fraud\".\n",
    "- Raising the threshold increases the precision but at the expense of lowering the recall. \n",
    "- At the extreme right, where the threshold is 1, we get into the situation where all the examples classified as \"fraud\" are actually \"fraud\"; we have no false positives. \n",
    "- Here we have a high precision but lower recall. \n",
    "- Usually the goal is to keep recall high as precision goes up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AP score \n",
    "\n",
    "- Often it's useful to have one number summarizing the PR plot (e.g., in hyperparameter optimization)\n",
    "- One way to do this is by computing the area under the PR curve. \n",
    "- This is called **average precision** (AP score)\n",
    "- AP score has a value between 0 (worst) and 1 (best). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the following handy function of `sklearn` to get the PR curve and the corresponding AP score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(pipe_lr, X_valid, y_valid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AP vs. F1-score\n",
    "\n",
    "It is very important to note this distinction:\n",
    "\n",
    "- F1 score is for a given threshold and measures the quality of `predict`.\n",
    "- AP score is a summary across thresholds and measures the quality of `predict_proba`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Remember to pick the desired threshold based on the results on the validation set and **not** on the test set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few comments on PR curve\n",
    "\n",
    "- Different classifiers might work well in different parts of the curve, i.e., at different operating points.   \n",
    "- We can compare PR curves of different classifiers to understand these differences. \n",
    "- Let's create PR curves for SVC and Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_svc = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get precision and recall for different thresholds? \n",
    "- Use the function `precision_recall_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Some more details\n",
    "\n",
    "- How are the thresholds and the precision and recall at the default threshold are calculated? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many thresholds? \n",
    "- It uses `n_thresholds` where `n_thresholds` is the number of unique `predict_proba` scores in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(pipe_lr.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each threshold, precision and recall are calculated.  \n",
    "- The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_lr.shape, precision_lr.shape, recall_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")\n",
    "precision_svc, recall_svc, thresholds_svc = precision_recall_curve(\n",
    "    y_valid, pipe_svc.decision_function(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, what's the index of the threshold that is closest to the default threshold of 0.5? \n",
    "- We are subtracting 0.5 from the thresholds so that \n",
    "    - the numbers close to 0 become -0.5\n",
    "    - the numbers close to 1 become 0.5    \n",
    "    - the numbers close to 0.5 become 0\n",
    "- After this transformation, we are interested in the threshold index where the number is close to 0. So we take  absolute values and argmin.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_default_lr = np.argmin(np.abs(thresholds_lr - 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC doesn't have `predict_proba`. Instead it has something called `decision_function`. The index of the threshold that is closest to 0 of decision function is the default threshold in SVC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_zero_svm = np.argmin(np.abs(thresholds_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR curves for logistic regression and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(precision_svc, recall_svc, label=\"svc\")\n",
    "plt.plot(precision_lr, recall_lr, label=\"logistic regression\")\n",
    "plt.plot(\n",
    "    precision_svc[close_zero_svm],\n",
    "    recall_svc[close_zero_svm],\n",
    "    \"o\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold svc\",\n",
    "    c=\"b\",\n",
    ")\n",
    "plt.plot(\n",
    "    precision_lr[close_default_lr],\n",
    "    recall_lr[close_default_lr],\n",
    "    \"*\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold logistic regression\",\n",
    "    c=\"r\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"best\", fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which model is doing better in this scenario: SVC or Logistic Regression? \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svc_preds = pipe_svc.predict(X_valid)\n",
    "lr_preds = pipe_lr.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"f1_score of logistic regression: {:.3f}\".format(f1_score(y_valid, lr_preds)))\n",
    "print(\"f1_score of svc: {:.3f}\".format(f1_score(y_valid, svc_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "ap_svc = average_precision_score(y_valid, pipe_svc.decision_function(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))\n",
    "print(\"Average precision of SVC: {:.3f}\".format(ap_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Comparing the precision-recall curves provide us a detail insight compared to f1 score.\n",
    "- For example, F1 scores for SVC and logistic regressions are pretty similar. In fact, f1 score of logistic regression is a tiny bit better. \n",
    "- But when we look at the PR curve, we see that SVC is doing better than logistic regression for most of the other thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receiver Operating Characteristic (ROC) curve \n",
    "\n",
    "- Another commonly used tool to analyze the behavior of classifiers at different thresholds.  \n",
    "- Similar to PR curve, it considers all possible thresholds for a given classifier given by `predict_proba` but instead of precision and recall it plots false positive rate (FPR) and true positive rate (TPR or recall).\n",
    "$$ TPR = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$ FPR  = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "- TPR $\\rightarrow$ Fraction of true positives out of all positive examples. \n",
    "- FPR $\\rightarrow$ Fraction of false positives out of all negative examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "default_threshold = np.argmin(np.abs(thresholds - 0.5))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[default_threshold],\n",
    "    tpr[default_threshold],\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Different points on the ROC curve represent different classification thresholds. The curve starts at (0,0) and ends at (1, 1).\n",
    "    - (0, 0) represents the threshold that classifies everything as the negative class\n",
    "    - (1, 1) represents the threshold that classifies everything as the positive class \n",
    "- The ideal curve is close to the top left\n",
    "    - Ideally, you want a classifier with high recall while keeping low false positive rate.  \n",
    "- The red dot corresponds to the threshold of 0.5, which is used by predict.\n",
    "- We see that compared to the default threshold, we can achieve a better recall of around 0.8 without increasing FPR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare ROC curve of different classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "\n",
    "fpr_svc, tpr_svc, thresholds_svc = roc_curve(\n",
    "    y_valid, pipe_svc.decision_function(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_default_lr = np.argmin(np.abs(thresholds_lr - 0.5))\n",
    "close_zero_svm = np.argmin(np.abs(thresholds_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr_svc, tpr_svc, label=\"svc\")\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"logistic regression\")\n",
    "plt.plot(\n",
    "    fpr_svc[close_zero_svm],\n",
    "    tpr_svc[close_zero_svm],\n",
    "    \"o\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold svc\",\n",
    "    c=\"b\",\n",
    ")\n",
    "plt.plot(\n",
    "    fpr_lr[close_default_lr],\n",
    "    tpr_lr[close_default_lr],\n",
    "    \"*\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold logistic regression\",\n",
    "    c=\"r\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate (Recall)\")\n",
    "plt.legend(loc=\"best\", fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Area under the curve (AUC)\n",
    "\n",
    "- AUC provides a single meaningful number for the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_lr = roc_auc_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "roc_svc = roc_auc_score(y_valid, pipe_svc.decision_function(X_valid))\n",
    "print(\"AUC for LR: {:.3f}\".format(roc_lr))\n",
    "print(\"AUC for SVC: {:.3f}\".format(roc_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- AUC of 0.5 means random chance. \n",
    "- AUC can be interpreted as evaluating the **ranking** of positive examples.\n",
    "- What's the probability that a randomly picked positive point has a higher score according to the classifier than a randomly picked point from the negative class. \n",
    "- AUC of 1.0 means all positive points have a higher score than all negative points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{important}\n",
    "For classification problems with imbalanced classes, using AP score or AUC is often much more meaningful than using accuracy. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `PrecisionRecallCurveDisplay`, there is a `RocCurveDisplay` function in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_estimator(pipe_lr, X_valid, y_valid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's look at all the scores at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scores = cross_validate(pipe, X_train_big, y_train_big, scoring=scoring)\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{seealso}\n",
    "Check out [these visualization](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation) on ROC and AUC.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{seealso}\n",
    "Check out how to plot ROC with cross-validation [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with class imbalance [[video](https://www.youtube.com/watch?v=jHaKRCFb6Qw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Class imbalance in training sets\n",
    "\n",
    "- This typically refers to having many more examples of one class than another in one's training set.\n",
    "- Real world data is often imbalanced. \n",
    "    - Our Credit Card Fraud dataset is imbalanced.\n",
    "    - Ad clicking data is usually drastically imbalanced. (Only around ~0.01% ads are clicked.)\n",
    "    - Spam classification datasets are also usually imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Addressing class imbalance\n",
    "A very important question to ask yourself: \"Why do I have a class imbalance?\"\n",
    "\n",
    "- Is it because one class is much more rare than the other?\n",
    "    - If it's just because one is more rare than the other, you need to ask whether you care about one type of error more than the other.    \n",
    "- Is it because of my data collection methods?\n",
    "    - If it's the data collection, then that means _your test and training data come from different distributions_!\n",
    "  \n",
    "In some cases, it may be fine to just ignore the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which type of error is more important? \n",
    "\n",
    "- False positives (FPs) and false negatives (FNs) have quite different real-world consequences. \n",
    "- In PR curve and ROC curve, we saw how changing the prediction threshold can change FPs and FNs. \n",
    "- We can then pick the threshold that's appropriate for our problem. \n",
    "- Example: if we want high recall, we may use a lower threshold (e.g., a threshold of 0.1). We'll then catch more fraudulent transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] > 0.10\n",
    "print(classification_report(y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handling imbalance\n",
    "\n",
    "Can we change the model itself rather than changing the threshold so that it takes into account the errors that are important to us?\n",
    "\n",
    "There are two common approaches for this: \n",
    "- **Changing the data (optional)** (not covered in this course)\n",
    "   - Undersampling\n",
    "   - Oversampling \n",
    "       - Random oversampling\n",
    "       - SMOTE \n",
    "- **Changing the training procedure** \n",
    "    - `class_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Changing the training procedure \n",
    "\n",
    "- All `sklearn` classifiers have a parameter called `class_weight`.\n",
    "- This allows you to specify that one class is more important than another.\n",
    "- For example, maybe a false negative is 10x more problematic than a false positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: `class_weight` parameter of `sklearn LogisticRegression` \n",
    "> class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, **class_weight=None**, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "> class_weight: dict or 'balanced', default=None\n",
    "\n",
    "> Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "url = \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\"\n",
    "IPython.display.IFrame(width=1000, height=650, src=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(\n",
    "#     pipe_lr,\n",
    "#     X_valid,\n",
    "#     y_valid,\n",
    "#     display_labels=[\"Non fraud\", \"fraud\"],\n",
    "#     values_format=\"d\",\n",
    "#     cmap=plt.cm.Blues,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lr, X_valid, y_valid, display_labels=[\"Non fraud\", \"fraud\"], values_format=\"d\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.named_steps[\"logisticregression\"].classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's set \"fraud\" class a weight of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_weight = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight={0: 1, 1: 10})\n",
    ")\n",
    "pipe_lr_weight.fit(X_train, y_train)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lr_weight,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Notice we've reduced false negatives and predicted more Fraud this time.\n",
    "- This was equivalent to saying give 10x more \"importance\" to fraud class. \n",
    "- Note that as a consequence we are also increasing false positives.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `class_weight=\"balanced\"`\n",
    "- A useful setting is `class_weight=\"balanced\"`.\n",
    "- This sets the weights so that the classes are \"equal\".\n",
    "\n",
    "> class_weight: dict, balanced or None\n",
    "If balanced, class weights will be given by n_samples / (n_classes * np.bincount(y)). If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.\n",
    "\n",
    "> sklearn.utils.class_weight.compute_class_weight(class_weight, classes, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_balanced = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    ")\n",
    "pipe_lr_balanced.fit(X_train, y_train)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lr_balanced,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    display_labels=[\"Non fraud\", \"fraud\"],\n",
    "    values_format=\"d\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced false negatives but we have many more false positives now ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Are we doing better with `class_weight=\"balanced\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dict = {}\n",
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500))\n",
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "orig_scores = cross_validate(pipe_lr, X_train_big, y_train_big, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_balanced = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    ")\n",
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "bal_scores = cross_validate(pipe_lr_balanced, X_train_big, y_train_big, scoring=scoring)\n",
    "comp_dict = {\n",
    "    \"Original\": pd.DataFrame(orig_scores).mean().tolist(),\n",
    "    \"class_weight='balanced'\": pd.DataFrame(bal_scores).mean().tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(comp_dict, index=bal_scores.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall is much better but precision has dropped a lot; we have many false positives. \n",
    "- You could also optimize `class_weight` using hyperparameter optimization for your specific problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Changing the class weight will **generally reduce accuracy**.\n",
    "- The original model was trying to maximize accuracy.\n",
    "- Now you're telling it to do something different.\n",
    "- But that can be fine, accuracy isn't the only metric that matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stratified Splits\n",
    "\n",
    "- A similar idea of \"balancing\" classes can be applied to data splits.\n",
    "- We have the same option in `train_test_split` with the `stratify` argument. \n",
    "- By default it splits the data so that if we have 10% negative examples in total, then each split will have 10% negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If you are carrying out cross validation using `cross_validate`, by default it uses [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html). From the documentation: \n",
    "\n",
    "> This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n",
    "\n",
    "- In other words, if we have 10% negative examples in total, then each fold will have 10% negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is stratifying a good idea? \n",
    "\n",
    "  - Well, it's no longer a random sample, which is probably theoretically bad, but not that big of a deal.\n",
    "  - If you have many examples, it shouldn't matter as much.\n",
    "  - It can be especially useful in multi-class, say if you have one class with very few cases.\n",
    "  - In general, these are difficult questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Optional) Changing the data \n",
    "\n",
    "- Undersampling\n",
    "- Oversampling \n",
    "   - Random oversampling\n",
    "   - SMOTE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We cannot use sklearn pipelines because of some API related problems. But there is something called [`imbalance learn`](https://imbalanced-learn.org/stable/), which is an extension of the `scikit-learn` API that allows us to resample. It's already in our course environment. If you don't have the course environment installed, you can install it in your environment with this command: \n",
    "\n",
    "`conda install -c conda-forge imbalanced-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_train_subsample, y_train_subsample = rus.fit_resample(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_subsample.shape)\n",
    "print(np.bincount(y_train_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_classes=2,\n",
    "    class_sep=2,\n",
    "    weights=[0.1, 0.9],\n",
    "    n_informative=3,\n",
    "    n_redundant=1,\n",
    "    flip_y=0,\n",
    "    n_features=20,\n",
    "    n_clusters_per_class=1,\n",
    "    n_samples=1000,\n",
    "    random_state=10,\n",
    ")\n",
    "print(\"Original dataset shape %s\" % Counter(y))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "print(\"Resampled dataset shape %s\" % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "undersample_pipe = make_imb_pipeline(\n",
    "    RandomUnderSampler(), StandardScaler(), LogisticRegression()\n",
    ")\n",
    "scores = cross_validate(\n",
    "    undersample_pipe, X_train, y_train, scoring=(\"roc_auc\", \"average_precision\")\n",
    ")\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Oversampling \n",
    "\n",
    "- Random oversampling with replacement \n",
    "- SMOTE: Synthetic Minority Over-sampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_train_oversample, y_train_oversample = ros.fit_resample(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_oversample.shape)\n",
    "print(np.bincount(y_train_oversample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "oversample_pipe = make_imb_pipeline(\n",
    "    RandomOverSampler(), StandardScaler(), LogisticRegression(max_iter=1000)\n",
    ")\n",
    "scores = cross_validate(\n",
    "    oversample_pipe, X_train, y_train, scoring=(\"roc_auc\", \"average_precision\")\n",
    ")\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/pdf/1106.1813.pdf)\n",
    "\n",
    "[sklearn SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html)\n",
    "\n",
    "- Create \"synthetic\" examples rather than by over-sampling with replacement.\n",
    "- Inspired by a technique of data augmentation that proved successful in handwritten character recognition. \n",
    "- The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the $k$ minority class nearest neighbors.\n",
    "- $k$ is chosen depending upon the amount of over-sampling required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### SMOTE idea \n",
    "\n",
    "- Take the difference between the feature vector (sample) under consideration and its nearest neighbor. \n",
    "- Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. \n",
    "- This causes the selection of a random point along the line segment between two specific features. \n",
    "- This approach effectively forces the decision region of the minority class to become more general.\n",
    "\n",
    "![](../../img/SMOTE_doccam.png)\n",
    "\n",
    "<!-- <img src=\"img/SMOTE_doccam.png\" width=\"600\" height=\"600\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using SMOTE\n",
    "\n",
    "- You need to [`imbalanced-learn`](https://imbalanced-learn.org/stable/index.html)\n",
    "> class imblearn.over_sampling.SMOTE(sampling_strategy='auto', random_state=None, k_neighbors=5, m_neighbors='deprecated', out_step='deprecated', kind='deprecated', svm_estimator='deprecated', n_jobs=1, ratio=None)\n",
    "\n",
    "> Class to perform over-sampling using SMOTE.\n",
    "\n",
    "> This object is an implementation of SMOTE - Synthetic Minority Over-sampling Technique as presented in [this paper](https://arxiv.org/pdf/1106.1813.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote_pipe = make_imb_pipeline(\n",
    "    SMOTE(), StandardScaler(), LogisticRegression(max_iter=1000)\n",
    ")\n",
    "scores = cross_validate(\n",
    "    smote_pipe, X_train, y_train, cv=10, scoring=(\"roc_auc\", \"average_precision\")\n",
    ")\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We got higher average precision score with SMOTE in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These are rather simple approaches to tackle class imbalance. \n",
    "- If you have a problem such as fraud detection problem where you want to spot rare events, you can think of this problem as anomaly detection problem and use algorithms such as isolation forests.\n",
    "- If you are interested in this area, it might be worth checking out this book on this topic. (I've not read it.) \n",
    "    - Imbalanced Learning: Foundations, Algorithms, and Applications\n",
    "    - It's available via UBC library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML fairness activity (~5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "AI/ML systems can give the illusion of objectivity as they are derived from seemingly unbiased data & algorithm. However, human are inherently biased and AI/ML systems, if not carefully evaluated, can even further amplify the existing inequities and systemic bias in our society.  \n",
    "\n",
    "How do we make sure our AI/ML systems are *fair*? Which metrics can we use to quantify 'fairness' in AI/ML systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine this on [the adult census data set](https://www.kaggle.com/uciml/adult-census-income). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "census_df = pd.read_csv(DATA_DIR + \"adult.csv\")\n",
    "census_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(census_df, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df_nan = train_df.replace(\"?\", np.nan)\n",
    "test_df_nan = test_df.replace(\"?\", np.nan)\n",
    "train_df_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's identify numeric and categorical features\n",
    "\n",
    "numeric_features = [\n",
    "    \"age\",\n",
    "    \"capital.gain\",\n",
    "    \"capital.loss\",\n",
    "    \"hours.per.week\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"marital.status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"native.country\",\n",
    "]\n",
    "\n",
    "ordinal_features = [\"education\"]\n",
    "binary_features = [\n",
    "    \"sex\"\n",
    "]  # Not binary in general but in this particular dataset it seems to have only two possible values\n",
    "drop_features = [\"education.num\", \"fnlwgt\"]\n",
    "target = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"education\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_levels = [\n",
    "    \"Preschool\",\n",
    "    \"1st-4th\",\n",
    "    \"5th-6th\",\n",
    "    \"7th-8th\",\n",
    "    \"9th\",\n",
    "    \"10th\",\n",
    "    \"11th\",\n",
    "    \"12th\",\n",
    "    \"HS-grad\",\n",
    "    \"Prof-school\",\n",
    "    \"Assoc-voc\",\n",
    "    \"Assoc-acdm\",\n",
    "    \"Some-college\",\n",
    "    \"Bachelors\",\n",
    "    \"Masters\",\n",
    "    \"Doctorate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(education_levels) == set(train_df[\"education\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df_nan.drop(columns=[target])\n",
    "y_train = train_df_nan[target]\n",
    "\n",
    "X_test = test_df_nan.drop(columns=[target])\n",
    "y_test = test_df_nan[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "numeric_transformer = make_pipeline(StandardScaler())\n",
    "\n",
    "ordinal_transformer = OrdinalEncoder(categories=[education_levels], dtype=int)\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    ")\n",
    "\n",
    "binary_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(drop=\"if_binary\", dtype=int),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer, ordinal_features),\n",
    "    (binary_transformer, binary_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(pipe_lr, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine confusion matrix separately for the two genders we have in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "preprocessor.named_transformers_[\"pipeline-2\"][\"onehotencoder\"].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_female = X_test.query(\"sex=='Female'\")  # X where sex is female\n",
    "X_male = X_test.query(\"sex=='Male'\")  # X where sex is male\n",
    "\n",
    "y_female = y_test[X_female.index]  # y where sex is female\n",
    "y_male = y_test[X_male.index]  # y where sex is male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get predictions for `X_female` and `y_male` with `pipe_lr`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_preds = pipe_lr.predict(X_female)\n",
    "male_preds = pipe_lr.predict(X_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the accuracy and confusion matrix for female and male classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_female, female_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_male, male_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot the female confusion matrix\n",
    "female_cm = ConfusionMatrixDisplay.from_estimator(pipe_lr, X_female, y_female, normalize=\"true\");\n",
    "axes[0].set_title('Confusion Matrix - Female');\n",
    "female_cm.plot(ax=axes[0]);\n",
    "\n",
    "\n",
    "# Plot the male confusion matrix\n",
    "male_cm = ConfusionMatrixDisplay.from_estimator(pipe_lr, X_male, y_male, normalize=\"true\");\n",
    "axes[1].set_title('Confusion Matrix - Male');\n",
    "male_cm.plot(ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "###  Questions for group discussion\n",
    "\n",
    "Let's assume that a company is using this classifier for loan approval with a simple rule that if the income is >=50K, approve the loan else reject the loan. \n",
    "\n",
    "In your group, discuss the questions below. \n",
    "\n",
    "1. Which group has a higher accuracy?\n",
    "2. Which group has a higher precision for class >50K? What about recall for class >50K?\n",
    "3. Will both groups have more or less the same proportion of people with approved loans? \n",
    "4. If a male and a female have both a certain level of income, will they have the same chance of getting the loan?\n",
    "5. Banks want to avoid approving unqualified applications (false positives) because default loan could have detrimental effects for them. Compare the false positive rates for the two groups.    \n",
    "6. Overall, do you think this income classifier will fairly treat both groups? What will be the consequences of using this classifier in loan approval application? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time permitting**\n",
    "1. Do you think the effect will still exist if the sex feature is removed from the model (but you still have it available separately to do the two confusion matrices)? \n",
    "2. Are there any other groups in this dataset worth examining for biases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- A number of possible ways to evaluate machine learning models \n",
    "    - Choose the evaluation metric that makes most sense in your context or which is most common in your discipline  \n",
    "- Two kinds of binary classification problems \n",
    "    - Distinguishing between two classes (e.g., dogs vs. cats)\n",
    "    - Spotting a class (e.g., spot fraud transaction, spot spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Precision, recall, f1-score are useful when dealing with spotting problems. \n",
    "- The thing that we are interested in spotting is considered \"positive\".   \n",
    "- Do you need to deal with class imbalance in the given problem? \n",
    "- Methods to deal with class imbalance \n",
    "    - Changing the training procedure \n",
    "        - `class_weight`\n",
    "    - Changing the data\n",
    "        - undersampling, oversampling, SMOTE\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do not blindly make decisions solely based on ML model predictions. \n",
    "- Try to carefully analyze the errors made by the model on certain groups.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relevant papers and resources \n",
    "\n",
    "- [The Relationship Between Precision-Recall and ROC Curves](https://www.biostat.wisc.edu/~page/rocpr.pdf)\n",
    "- [Article claiming that PR curve are better than ROC for imbalanced datasets](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)\n",
    "- [Precision-Recall-Gain Curves: PR Analysis Done Right](https://papers.nips.cc/paper/2015/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf)\n",
    "- [ROC animation](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation)\n",
    "- [Generalization in Adaptive Data Analysis and Holdout Reuse](https://arxiv.org/pdf/1506.02629.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
